<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_bzn6d03kha9x-0.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-0 0}.lst-kix_bzn6d03kha9x-0>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-0}ol.lst-kix_qk314e4kerqq-0.start{counter-reset:lst-ctn-kix_qk314e4kerqq-0 0}.lst-kix_qk314e4kerqq-4>li{counter-increment:lst-ctn-kix_qk314e4kerqq-4}ol.lst-kix_qbrvifn5kda-1.start{counter-reset:lst-ctn-kix_qbrvifn5kda-1 0}.lst-kix_ydtm4ehjpy04-3>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-3}.lst-kix_qbrvifn5kda-2>li{counter-increment:lst-ctn-kix_qbrvifn5kda-2}.lst-kix_qk314e4kerqq-5>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-5,lower-roman) ". "}.lst-kix_qk314e4kerqq-7>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-7,lower-latin) ". "}ol.lst-kix_ydtm4ehjpy04-5.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-5 0}.lst-kix_qk314e4kerqq-6>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-6,decimal) ". "}ol.lst-kix_qbrvifn5kda-7.start{counter-reset:lst-ctn-kix_qbrvifn5kda-7 0}.lst-kix_qk314e4kerqq-8>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-8,lower-roman) ". "}.lst-kix_bzn6d03kha9x-2>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-2}.lst-kix_ydtm4ehjpy04-5>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-5}ol.lst-kix_bzn6d03kha9x-6.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-6 0}ol.lst-kix_bzn6d03kha9x-5.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-5 0}.lst-kix_qk314e4kerqq-0>li{counter-increment:lst-ctn-kix_qk314e4kerqq-0}.lst-kix_qk314e4kerqq-3>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-3,decimal) ". "}ol.lst-kix_qk314e4kerqq-6.start{counter-reset:lst-ctn-kix_qk314e4kerqq-6 0}.lst-kix_qk314e4kerqq-4>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-4,lower-latin) ". "}.lst-kix_qk314e4kerqq-6>li{counter-increment:lst-ctn-kix_qk314e4kerqq-6}ol.lst-kix_qk314e4kerqq-1.start{counter-reset:lst-ctn-kix_qk314e4kerqq-1 0}.lst-kix_ydtm4ehjpy04-1>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-1}.lst-kix_qk314e4kerqq-2>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-2,lower-roman) ". "}.lst-kix_qk314e4kerqq-1>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-1,lower-latin) ". "}ol.lst-kix_qbrvifn5kda-2.start{counter-reset:lst-ctn-kix_qbrvifn5kda-2 0}.lst-kix_qk314e4kerqq-0>li:before{content:"" counter(lst-ctn-kix_qk314e4kerqq-0,decimal) ". "}ol.lst-kix_bzn6d03kha9x-8{list-style-type:none}ol.lst-kix_bzn6d03kha9x-7{list-style-type:none}ol.lst-kix_bzn6d03kha9x-6{list-style-type:none}.lst-kix_qbrvifn5kda-4>li{counter-increment:lst-ctn-kix_qbrvifn5kda-4}ol.lst-kix_bzn6d03kha9x-5{list-style-type:none}ol.lst-kix_bzn6d03kha9x-4{list-style-type:none}ol.lst-kix_bzn6d03kha9x-3{list-style-type:none}ol.lst-kix_bzn6d03kha9x-2{list-style-type:none}ol.lst-kix_bzn6d03kha9x-1{list-style-type:none}ol.lst-kix_bzn6d03kha9x-0{list-style-type:none}ol.lst-kix_qbrvifn5kda-8.start{counter-reset:lst-ctn-kix_qbrvifn5kda-8 0}ol.lst-kix_qk314e4kerqq-7.start{counter-reset:lst-ctn-kix_qk314e4kerqq-7 0}ol.lst-kix_ydtm4ehjpy04-0{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-1.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-1 0}ol.lst-kix_bzn6d03kha9x-7.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-7 0}ol.lst-kix_ydtm4ehjpy04-1{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-2{list-style-type:none}.lst-kix_ydtm4ehjpy04-0>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-0,decimal) ". "}.lst-kix_bzn6d03kha9x-1>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-1,lower-latin) ". "}.lst-kix_u40ivt6j8ps6-3>li:before{content:"-  "}.lst-kix_ydtm4ehjpy04-7>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-7}ol.lst-kix_qbrvifn5kda-0.start{counter-reset:lst-ctn-kix_qbrvifn5kda-0 0}.lst-kix_u40ivt6j8ps6-1>li:before{content:"-  "}.lst-kix_u40ivt6j8ps6-5>li:before{content:"-  "}.lst-kix_bzn6d03kha9x-5>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-5,lower-roman) ". "}.lst-kix_qbrvifn5kda-7>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-7,lower-latin) ". "}ul.lst-kix_ls8pjoc62kvq-8{list-style-type:none}.lst-kix_bzn6d03kha9x-7>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-7,lower-latin) ". "}ul.lst-kix_ls8pjoc62kvq-6{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-7{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-4{list-style-type:none}.lst-kix_bzn6d03kha9x-5>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-5}ul.lst-kix_ls8pjoc62kvq-5{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-2{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-3{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-0{list-style-type:none}ul.lst-kix_ls8pjoc62kvq-1{list-style-type:none}.lst-kix_qbrvifn5kda-0>li{counter-increment:lst-ctn-kix_qbrvifn5kda-0}ol.lst-kix_ydtm4ehjpy04-3{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-4{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-5{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-6{list-style-type:none}.lst-kix_bzn6d03kha9x-3>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-3,decimal) ". "}ol.lst-kix_ydtm4ehjpy04-7{list-style-type:none}ol.lst-kix_ydtm4ehjpy04-8{list-style-type:none}.lst-kix_h256d4t1o6ma-4>li:before{content:"\0025cb  "}.lst-kix_h256d4t1o6ma-8>li:before{content:"\0025a0  "}.lst-kix_ydtm4ehjpy04-0>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-0}.lst-kix_qbrvifn5kda-6>li{counter-increment:lst-ctn-kix_qbrvifn5kda-6}.lst-kix_h256d4t1o6ma-6>li:before{content:"\0025cf  "}.lst-kix_u40ivt6j8ps6-7>li:before{content:"-  "}.lst-kix_ls8pjoc62kvq-4>li:before{content:"\0025cb  "}ol.lst-kix_ydtm4ehjpy04-0.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-0 0}.lst-kix_qk314e4kerqq-5>li{counter-increment:lst-ctn-kix_qk314e4kerqq-5}ul.lst-kix_h256d4t1o6ma-6{list-style-type:none}ul.lst-kix_h256d4t1o6ma-7{list-style-type:none}ul.lst-kix_h256d4t1o6ma-8{list-style-type:none}ul.lst-kix_h256d4t1o6ma-2{list-style-type:none}ul.lst-kix_h256d4t1o6ma-3{list-style-type:none}.lst-kix_ls8pjoc62kvq-6>li:before{content:"\0025cf  "}ul.lst-kix_h256d4t1o6ma-4{list-style-type:none}.lst-kix_bzn6d03kha9x-4>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-4}ul.lst-kix_h256d4t1o6ma-5{list-style-type:none}ol.lst-kix_bzn6d03kha9x-8.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-8 0}.lst-kix_ls8pjoc62kvq-8>li:before{content:"\0025a0  "}ul.lst-kix_h256d4t1o6ma-0{list-style-type:none}ul.lst-kix_h256d4t1o6ma-1{list-style-type:none}.lst-kix_ydtm4ehjpy04-6>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-6}.lst-kix_h256d4t1o6ma-0>li:before{content:"\0025cf  "}.lst-kix_h256d4t1o6ma-2>li:before{content:"\0025a0  "}.lst-kix_qbrvifn5kda-3>li{counter-increment:lst-ctn-kix_qbrvifn5kda-3}ol.lst-kix_qbrvifn5kda-3{list-style-type:none}ol.lst-kix_qbrvifn5kda-2{list-style-type:none}ol.lst-kix_qbrvifn5kda-1{list-style-type:none}ol.lst-kix_qbrvifn5kda-0{list-style-type:none}.lst-kix_qk314e4kerqq-3>li{counter-increment:lst-ctn-kix_qk314e4kerqq-3}ol.lst-kix_qk314e4kerqq-3.start{counter-reset:lst-ctn-kix_qk314e4kerqq-3 0}ol.lst-kix_qbrvifn5kda-4.start{counter-reset:lst-ctn-kix_qbrvifn5kda-4 0}.lst-kix_xvwfwnxcnqtq-1>li:before{content:"\0025cb  "}.lst-kix_ls8pjoc62kvq-3>li:before{content:"\0025cf  "}ul.lst-kix_xvwfwnxcnqtq-6{list-style-type:none}ul.lst-kix_xvwfwnxcnqtq-7{list-style-type:none}ul.lst-kix_xvwfwnxcnqtq-8{list-style-type:none}.lst-kix_xvwfwnxcnqtq-2>li:before{content:"\0025a0  "}.lst-kix_ls8pjoc62kvq-1>li:before{content:"\0025cb  "}ul.lst-kix_xvwfwnxcnqtq-2{list-style-type:none}.lst-kix_qbrvifn5kda-5>li{counter-increment:lst-ctn-kix_qbrvifn5kda-5}ul.lst-kix_xvwfwnxcnqtq-3{list-style-type:none}.lst-kix_ls8pjoc62kvq-2>li:before{content:"\0025a0  "}ul.lst-kix_xvwfwnxcnqtq-4{list-style-type:none}.lst-kix_xvwfwnxcnqtq-3>li:before{content:"\0025cf  "}ul.lst-kix_xvwfwnxcnqtq-5{list-style-type:none}ol.lst-kix_qk314e4kerqq-8.start{counter-reset:lst-ctn-kix_qk314e4kerqq-8 0}.lst-kix_xvwfwnxcnqtq-4>li:before{content:"\0025cb  "}.lst-kix_xvwfwnxcnqtq-6>li:before{content:"\0025cf  "}ol.lst-kix_ydtm4ehjpy04-2.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-2 0}.lst-kix_ls8pjoc62kvq-0>li:before{content:"\0025cf  "}.lst-kix_xvwfwnxcnqtq-5>li:before{content:"\0025a0  "}ul.lst-kix_u40ivt6j8ps6-2{list-style-type:none}ol.lst-kix_bzn6d03kha9x-3.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-3 0}ul.lst-kix_u40ivt6j8ps6-3{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-0{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-1{list-style-type:none}.lst-kix_xvwfwnxcnqtq-8>li:before{content:"\0025a0  "}.lst-kix_ydtm4ehjpy04-2>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-2}.lst-kix_xvwfwnxcnqtq-7>li:before{content:"\0025cb  "}.lst-kix_qk314e4kerqq-1>li{counter-increment:lst-ctn-kix_qk314e4kerqq-1}ol.lst-kix_ydtm4ehjpy04-8.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-8 0}ul.lst-kix_u40ivt6j8ps6-8{list-style-type:none}ul.lst-kix_xvwfwnxcnqtq-0{list-style-type:none}ul.lst-kix_xvwfwnxcnqtq-1{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-6{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-7{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-4{list-style-type:none}ul.lst-kix_u40ivt6j8ps6-5{list-style-type:none}.lst-kix_bzn6d03kha9x-3>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-3}.lst-kix_qk314e4kerqq-7>li{counter-increment:lst-ctn-kix_qk314e4kerqq-7}.lst-kix_bzn6d03kha9x-8>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-8,lower-roman) ". "}.lst-kix_qbrvifn5kda-6>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-6,decimal) ". "}.lst-kix_qbrvifn5kda-5>li:before{content:"(" counter(lst-ctn-kix_qbrvifn5kda-5,lower-roman) ") "}.lst-kix_qbrvifn5kda-4>li:before{content:"(" counter(lst-ctn-kix_qbrvifn5kda-4,lower-latin) ") "}.lst-kix_ydtm4ehjpy04-4>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-4}.lst-kix_qbrvifn5kda-1>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-1,lower-latin) ") "}.lst-kix_qbrvifn5kda-3>li:before{content:"(" counter(lst-ctn-kix_qbrvifn5kda-3,decimal) ") "}.lst-kix_qbrvifn5kda-2>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-2,lower-roman) ") "}ol.lst-kix_bzn6d03kha9x-4.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-4 0}.lst-kix_ydtm4ehjpy04-7>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-7,lower-latin) ". "}.lst-kix_ydtm4ehjpy04-8>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-8,lower-roman) ". "}.lst-kix_qbrvifn5kda-0>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-0,decimal) ") "}ol.lst-kix_ydtm4ehjpy04-3.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-3 0}.lst-kix_ydtm4ehjpy04-3>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-3,decimal) ". "}.lst-kix_ydtm4ehjpy04-4>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-4,lower-latin) ". "}.lst-kix_qbrvifn5kda-1>li{counter-increment:lst-ctn-kix_qbrvifn5kda-1}.lst-kix_ydtm4ehjpy04-1>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-1,lower-latin) ". "}.lst-kix_ydtm4ehjpy04-2>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-2,lower-roman) ". "}.lst-kix_ydtm4ehjpy04-5>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-5,lower-roman) ". "}.lst-kix_ydtm4ehjpy04-6>li:before{content:"" counter(lst-ctn-kix_ydtm4ehjpy04-6,decimal) ". "}.lst-kix_qbrvifn5kda-7>li{counter-increment:lst-ctn-kix_qbrvifn5kda-7}.lst-kix_bzn6d03kha9x-6>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-6}.lst-kix_ydtm4ehjpy04-8>li{counter-increment:lst-ctn-kix_ydtm4ehjpy04-8}ol.lst-kix_ydtm4ehjpy04-4.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-4 0}ol.lst-kix_qk314e4kerqq-4{list-style-type:none}ol.lst-kix_qk314e4kerqq-5{list-style-type:none}ol.lst-kix_qk314e4kerqq-2{list-style-type:none}.lst-kix_u40ivt6j8ps6-2>li:before{content:"-  "}.lst-kix_u40ivt6j8ps6-4>li:before{content:"-  "}ol.lst-kix_qk314e4kerqq-3{list-style-type:none}.lst-kix_qbrvifn5kda-8>li{counter-increment:lst-ctn-kix_qbrvifn5kda-8}ol.lst-kix_qk314e4kerqq-8{list-style-type:none}.lst-kix_bzn6d03kha9x-0>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-0,decimal) ". "}.lst-kix_bzn6d03kha9x-4>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-4,lower-latin) ". "}ol.lst-kix_qk314e4kerqq-6{list-style-type:none}ol.lst-kix_qk314e4kerqq-7{list-style-type:none}ol.lst-kix_qk314e4kerqq-0{list-style-type:none}ol.lst-kix_qk314e4kerqq-1{list-style-type:none}.lst-kix_bzn6d03kha9x-6>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-6,decimal) ". "}.lst-kix_qbrvifn5kda-8>li:before{content:"" counter(lst-ctn-kix_qbrvifn5kda-8,lower-roman) ". "}.lst-kix_bzn6d03kha9x-8>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-8}ol.lst-kix_qbrvifn5kda-3.start{counter-reset:lst-ctn-kix_qbrvifn5kda-3 0}ol.lst-kix_qk314e4kerqq-2.start{counter-reset:lst-ctn-kix_qk314e4kerqq-2 0}.lst-kix_u40ivt6j8ps6-0>li:before{content:"-  "}.lst-kix_bzn6d03kha9x-2>li:before{content:"" counter(lst-ctn-kix_bzn6d03kha9x-2,lower-roman) ". "}ol.lst-kix_ydtm4ehjpy04-6.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-6 0}.lst-kix_h256d4t1o6ma-7>li:before{content:"\0025cb  "}ol.lst-kix_qk314e4kerqq-5.start{counter-reset:lst-ctn-kix_qk314e4kerqq-5 0}.lst-kix_h256d4t1o6ma-5>li:before{content:"\0025a0  "}ol.lst-kix_qbrvifn5kda-6.start{counter-reset:lst-ctn-kix_qbrvifn5kda-6 0}.lst-kix_u40ivt6j8ps6-6>li:before{content:"-  "}.lst-kix_u40ivt6j8ps6-8>li:before{content:"-  "}ol.lst-kix_bzn6d03kha9x-2.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-2 0}.lst-kix_xvwfwnxcnqtq-0>li:before{content:"\0025cf  "}.lst-kix_ls8pjoc62kvq-5>li:before{content:"\0025a0  "}ol.lst-kix_ydtm4ehjpy04-7.start{counter-reset:lst-ctn-kix_ydtm4ehjpy04-7 0}ol.lst-kix_qbrvifn5kda-5.start{counter-reset:lst-ctn-kix_qbrvifn5kda-5 0}.lst-kix_bzn6d03kha9x-7>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-7}.lst-kix_qk314e4kerqq-8>li{counter-increment:lst-ctn-kix_qk314e4kerqq-8}.lst-kix_bzn6d03kha9x-1>li{counter-increment:lst-ctn-kix_bzn6d03kha9x-1}.lst-kix_ls8pjoc62kvq-7>li:before{content:"\0025cb  "}.lst-kix_qk314e4kerqq-2>li{counter-increment:lst-ctn-kix_qk314e4kerqq-2}ol.lst-kix_qk314e4kerqq-4.start{counter-reset:lst-ctn-kix_qk314e4kerqq-4 0}ol.lst-kix_qbrvifn5kda-7{list-style-type:none}ol.lst-kix_qbrvifn5kda-6{list-style-type:none}ol.lst-kix_qbrvifn5kda-5{list-style-type:none}.lst-kix_h256d4t1o6ma-1>li:before{content:"\0025cb  "}ol.lst-kix_qbrvifn5kda-4{list-style-type:none}ol.lst-kix_qbrvifn5kda-8{list-style-type:none}.lst-kix_h256d4t1o6ma-3>li:before{content:"\0025cf  "}ol.lst-kix_bzn6d03kha9x-1.start{counter-reset:lst-ctn-kix_bzn6d03kha9x-1 0}ol{margin:0;padding:0}table td,table th{padding:0}.c14{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:64.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c26{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:70.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c25{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:145.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c24{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:53.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c17{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:56.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c20{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:44.2pt;border-top-color:#cccccc;border-bottom-style:solid}.c0{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c39{margin-left:36pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c7{margin-left:54pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c22{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c38{padding-top:10pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{padding-top:10pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c34{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c43{padding-top:4pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c47{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:left}.c45{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;text-align:left}.c37{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c10{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c11{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c41{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c31{color:#434343;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c48{color:#666666;text-decoration:none;vertical-align:baseline;font-size:15pt;font-style:normal}.c27{color:#000000;text-decoration:none;vertical-align:baseline;font-size:20pt;font-style:normal}.c32{color:#000000;text-decoration:none;vertical-align:baseline;font-size:26pt;font-style:normal}.c40{color:#666666;text-decoration:none;vertical-align:baseline;font-size:12pt;font-style:normal}.c36{color:#000000;text-decoration:none;vertical-align:baseline;font-size:16pt;font-style:normal}.c33{border-spacing:0;border-collapse:collapse;margin-right:auto}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:right}.c18{margin-left:44pt;text-indent:-44pt;orphans:2;widows:2}.c16{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c35{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c51{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c30{font-weight:400;font-family:"Arial"}.c21{font-weight:700;font-family:"Times New Roman"}.c2{font-weight:400;font-family:"Times New Roman"}.c9{color:inherit;text-decoration:inherit}.c42{padding:0;margin:0}.c29{margin-left:36pt;padding-left:0pt}.c44{font-size:11pt}.c46{font-size:20pt}.c4{height:11pt}.c50{height:17pt}.c15{font-style:italic}.c28{background-color:#ffff00}.c19{height:15pt}.c49{font-size:13pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c51"><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 226.74px; height: 62.50px;"><img alt="" src="images/image11.jpg" style="width: 226.74px; height: 62.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c5 c21">&nbsp;</span></p><p class="c4 c47 title" id="h.ye4e3kae0r6d"><span class="c32 c2"></span></p><p class="c47 title" id="h.rzeegxw7mhqw"><span class="c2">Humanitarian High Resolution </span><span class="c2">Mapping</span></p><p class="c45 subtitle" id="h.30fhpmdvurek"><span class="c2 c48">Complementing Assessments with Remote Sensing Open Data</span></p><p class="c3 c4"><span class="c5 c21"></span></p><h1 class="c37" id="h.dx29uol96625"><span class="c2 c27">Table of Contents</span></h1><p class="c43"><span class="c16 c2"><a class="c9" href="#h.dx29uol96625">Table of Contents</a></span></p><p class="c38"><span class="c16 c2"><a class="c9" href="#h.ynasm68w8elb">1 Relevance</a></span></p><p class="c38"><span class="c16 c2"><a class="c9" href="#h.l0xq1bezdgf7">3 Overview of our approach</a></span></p><p class="c38"><span class="c16 c2"><a class="c9" href="#h.wuf7cbh0yoe9">4 Sources of Information</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.1g0vftrc668b">4.1 Ground Truth</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.qctiwc8ileu">4.2 Features extraction from open sources</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.jk1dfrdnz1t">4.2.1 Transfer Learning</a></span></p><p class="c7"><span class="c16 c2"><a class="c9" href="#h.ra31jgogyz1j">4.2.1.1 Training the CNN on nightlights data</a></span></p><p class="c7"><span class="c16 c2"><a class="c9" href="#h.2wn1j0yr1ngm">4.2.1.2 Features Extraction</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.jcshz3fttdxm">4.3 OpenStreetMap features</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.ln1b8ngf4cm">4.4 Remote Sensing Indices</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.nfbkw7inunoa">4.5 Nightlights</a></span></p><p class="c38"><span class="c16 c2"><a class="c9" href="#h.33mrj8ohbtxh">5 Results</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.qty3w9bhp6hu">5.1 Problems and Limitations</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.1s4nh232cr1y">5.1.1 Ground truth data</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.wbrh31ps82uo">5.1.2 Data Homogeneity</a></span></p><p class="c22"><span class="c16 c2"><a class="c9" href="#h.pwg080ld9wpj">5.2 Future Work</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.hx8re82vlwky">5.2.1 Open Source and Productionisation</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.unpu9vo28qr5">5.2.2 Better Proxy for Poverty</a></span></p><p class="c39"><span class="c16 c2"><a class="c9" href="#h.ntoo0tydj8ah">5.2.3 Gaussian Processes?</a></span></p><p class="c13"><span class="c16 c2"><a class="c9" href="#h.bnjyuxowbqcq">References</a></span></p><h1 class="c37" id="h.ynasm68w8elb"><span class="c2">1 Relevance </span><span class="c27 c2">&nbsp;</span></h1><p class="c3"><span class="c2">Being able to monitor the food security </span><span class="c2">situation </span><span class="c2">is a crucial condition for reducing hunger. For this reason</span><span class="c2">,</span><span class="c2">&nbsp;the World Food Programme</span><span class="c2">&nbsp;(WFP)</span><span class="c2">&nbsp;is continuously conducting </span><span class="c2">household </span><span class="c2">surveys. </span><span class="c2">However, t</span><span class="c2">he difficulties and cost of collecting face-to-face data in remote or unsafe areas mean that the estimates are only representative at a low resolution - usually regional or district level aggregation. </span><span class="c2">In order to allocate resources more efficiently, </span><span class="c2">WFP and other humanitarian actors need more detailed maps</span><span class="c2">.</span><span class="c5 c2">&nbsp;</span></p><p class="c3"><span class="c5 c2">&nbsp;</span></p><p class="c3"><span class="c2">The main aim of our initiative is to </span><span class="c2">leverage </span><span class="c2">open</span><span class="c2">&nbsp;geospatial data for use in WFP and </span><span class="c2">other </span><span class="c2">humanitarian sector assessments and to make it accessible for a broad range of users. In WFP&rsquo;s context this means enabling users to produce fine-scale food security maps. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c5 c2">Our approach builds on recent findings by a research group from Stanford on how machine learning and high-resolution satellite images can be used in combination with survey data to predict poverty indicators for small areas (Head, Manguin, Tran, &amp; Blumenstock, 2017; Jean et al., 2016). We tested our approach for a variety of country case studies based on World Bank as well as World Food Programme survey data. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">The results contribute to the existing literature with regards to three main points: First, by combining further valuable open-source data, such as OpenStreetMap information and nightlights, with the satellite data-based image recognition, and weighting it by population data, we are able to further refine prediction results for poverty indicators. Second, by applying the model to food security indicators, we broaden the usage of high-resolution mapping to humanitarian actors. Third, we increase the operationalization of research results by making results easily accessible to policy makers through a web-based application. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2 c46">2 Existing Literature on High Spatial Resolution Mapping</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Different approaches have been used to map socio-economic indicators at a high spatial resolution</span><span class="c5 c2">. These approaches generally rely on survey data as ground truth to estimate and evaluate statistical models and they mostly focus on poverty indicators. However, the set of covariates and the techniques employed to extract these covariates vary broadly. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">The most common statistical technique is referred </span><span class="c2">to </span><span class="c2">as </span><span class="c2 c15">Small Area Estimates</span><span class="c2">&nbsp;(SAE, Elbers, Lanjouw, &amp; Lanjouw, 2003). It consists in fitting a model linking the target variable of interest with a set of relevant covariates collected through survey data and applying this model to the same set of covariates in census data (where the target variable was not measured). Census population counts usually being available at a very small administrative level, the resulting predictions downscale the results. WFP, for example, produced small-area estimations of food security indicators in Bangladesh </span><span class="c2 c28">(Jones and Haslett, 2003),</span><span class="c2">&nbsp;Nepal </span><span class="c2 c28">(Jones and Haslett, 2006 and 2013)</span><span class="c2">&nbsp;and Cambodia </span><span class="c2 c28">(Haslett, Jones and Sefton, 2013). </span><span class="c2">This</span><span class="c2">&nbsp;technique, however is difficult to apply in Sub-Saharan Africa as it requires recent and reliable census data. It also relies on the assumption that the set of covariates </span><span class="c2">was measured </span><span class="c2">in </span><span class="c5 c2">the same way in both the survey and the census data. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Whe</span><span class="c2">re</span><span class="c2">&nbsp;no census data </span><span class="c2">was</span><span class="c2">&nbsp;available, researchers have been using geospatial data to make predictions at a high spatial resolution. Geospatial data is gridded data of diverse covariates</span><span class="c2">, </span><span class="c2">such as climate, accessibility, environment or topographic features. These gridded covariates are often derived from satellite imager</span><span class="c2">y</span><span class="c2">&nbsp;and are</span><span class="c2">, therefore,</span><span class="c2">&nbsp;available globally at </span><span class="c2">a </span><span class="c2">high resolution. </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=http://www.worldpop.org.uk/&amp;sa=D&amp;ust=1534154057927000">WorldPop</a></span><span class="c2">&nbsp;researchers have found that these geospatial covariates </span><span class="c2">are able to predict </span><span class="c2">the geographic distribution </span><span class="c2">of population</span><span class="c2">&nbsp;and population characteristics (age, </span><span class="c2">births,</span><span class="c2">&nbsp;etc.)</span><span class="c2">&nbsp;well</span><span class="c2">. In particular, models trained on census population counts were able to</span><span class="c2 c16"><a class="c9" href="https://www.google.com/url?q=http://www.worldpop.org.uk/about_our_work/projects/index.php?sheet%3DBottom-up-age-structures&amp;sa=D&amp;ust=1534154057928000">&nbsp;map population distribution at 100m</a></span><span class="c5 c2">&nbsp;resolution in the majority of countries in the world (Alegana et al., 2015; Tatem, 2014). </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Geospatial data has also been used to model </span><span class="c2">gender-disaggregated development indicators in four African countries as well as </span><span class="c2">poverty indicators in </span><span class="c2">a variety of</span><span class="c2">&nbsp;countries</span><span class="c2">.</span><span class="c2">&nbsp;The ground truth data comes from </span><span class="c2">the Demographic and Health Surveys Program (</span><span class="c2">DHS</span><span class="c2">)</span><span class="c2">&nbsp;and</span><span class="c2">&nbsp;the World Bank&rsquo;s Living Standards Measurement Study</span><span class="c2">&nbsp;</span><span class="c2">(</span><span class="c2">LSMS</span><span class="c2">). Their</span><span class="c2">&nbsp;assessments are linked to geographic covariates with the survey GPS coordinates available at the cluster level. This method is referred as the </span><span class="c2 c15">Bottom-Up</span><span class="c2 c5">&nbsp;approach. In a study in Bangladesh, Steele et al. (2017) also used mobile-phone metadata features available at the cell-phone tower level were also used as additional covariates to predict poverty.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Finally, a group at Stanford University (Jean et al., 2016) &nbsp;has used image recognition machine learning models to extract features from high-resolution satellite images. They predicted poverty indicators (wealth index and expenditures) in five African countries with the extracted features. This technique known as </span><span class="c2 c15">Transfer Learning</span><span class="c5 c2">&nbsp;is complex: Convolutional Neural Networks are trained to classify nightlight intensity from the satellite images and features are extracted from the intermediate layers of the trained networks. Those features are then fed into a traditional linear model. The ground truth data is also LSMS and DHS data aggregated at the survey cluster location. This approach has since been replicated with a broader set of indicators and countries (Head, Manguin, Tran, &amp; Blumenstock, 2017) . </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">The main aim of this paper is to contribute to the existing research by trying to unify the different approaches in an attempt to create an </span><span class="c2">automatic process </span><span class="c2">that would apply on various food security indicators collected by WFP. We have selected data sources that are globally and programmatically available and built a pipeline to process data and extract covariates. More specifically, we have innovated the current approaches by</span><span class="c2">: </span><span class="c5 c2">&nbsp;</span></p><p class="c3 c4"><span class="c5 c2"></span></p><ol class="c42 lst-kix_ydtm4ehjpy04-0 start" start="1"><li class="c3 c29"><span class="c2">Testing high resolution mapping on </span><span class="c2">WFP core </span><span class="c2">food security indicators (</span><span class="c2">Food Consumption Score (</span><span class="c2">FCS</span><span class="c2">)</span><span class="c5 c2">&nbsp;and Food Expenditures)</span></li><li class="c3 c29"><span class="c2">Combining the &ldquo;transfer learning approach&rdquo; with geographic gridded data to extract a </span><span class="c2">broader </span><span class="c2">set of features that </span><span class="c2">yield</span><span class="c5 c2">&nbsp;more stable results. </span></li><li class="c3 c29"><span class="c2">Adapting </span><span class="c2">the </span><span class="c2">&ldquo;transfer learning approach&rdquo; to use different source</span><span class="c2">s</span><span class="c2">&nbsp;of imager</span><span class="c2">y</span><span class="c2">&nbsp;available more frequently (Sentinel 2) and train</span><span class="c2">ing</span><span class="c5 c2">&nbsp;lighter neural networks to increase computational efficiency. </span></li><li class="c3 c29"><span class="c2">Adapting the &ldquo;geographic data models&rdquo; </span><span class="c2">to </span><span class="c2">programmatically </span><span class="c2">extract</span><span class="c5 c2">&nbsp;gridded covariates from various open data sources with a given set of parametres. </span></li><li class="c3 c29"><span class="c2">Building a </span><span class="c2">simple </span><span class="c5 c2">prediction ensemble model that averages predictions from a linear regression model and a k-Nearest Neighbor regression model to account for the spatial autocorrelation. </span></li><li class="c3 c29"><span class="c5 c2">Developing an open source modular code-base to replicate research and allows for adding new features and models.</span></li><li class="c3 c29"><span class="c2">Creating a user-friendly web application </span><span class="c2">which allows</span><span class="c2">&nbsp;users to evaluate the method on their own geo-located dataset and automatically map the</span><span class="c2">ir</span><span class="c5 c2">&nbsp;prediction results. </span></li></ol><h1 class="c37" id="h.l0xq1bezdgf7"><span class="c2">3</span><span class="c2">&nbsp;Overview of our approach</span></h1><p class="c3"><span class="c5 c2">&nbsp;</span></p><p class="c3"><span class="c2">Humanitarian High Resolution Mapping is about developing an application which automatically draws on various open data sources/streams combined with survey data to produce detailed maps of poverty and food security. Machine learning is used to optimize the models that describe the relationships between the survey indicators and the various features we extract from the open data sources are. Once the models are trained, we use them to make predictions for areas where no survey data was collected (out</span><span class="c2">-</span><span class="c2">of</span><span class="c2">-</span><span class="c2">sample predictions) and to </span><span class="c5 c2">visualize those predictions in maps.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">To date, we have deployed a first version of a simple web application for internal testing that is able to automatically downscale geo-referenced survey data. </span><span class="c2">Geographic coordinates are increasingly being recorded when household surveys are conducted and t</span><span class="c2">his application enables users to easily upload geo-referenced survey data and make predictions for areas where no survey data is available.</span><span class="c2">&nbsp;It produces a raster that can then be re-aggregated to a given administrative level. Once the data is uploaded</span><span class="c2">,</span><span class="c2">&nbsp;the system automatically pulls covariate data that is relevant to the area and </span><span class="c2">time of interest</span><span class="c5 c2">&nbsp;from:</span></p><ul class="c42 lst-kix_u40ivt6j8ps6-0 start"><li class="c3 c29"><span class="c5 c2">Sentinel 2 (Copernicus / ESA)</span></li><li class="c3 c29"><span class="c5 c2">Google Maps (Static API)</span></li><li class="c3 c29"><span class="c5 c2">OpenStreetMap (Overpass API)</span></li><li class="c3 c29"><span class="c5 c2">Nightlights (NOAA VIIRS) </span></li></ul><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Our method is described in Section 3 and t</span><span class="c2">he individual sources are described in greater detail in Section 4. The graph below gives an overview of the interaction between the different sources.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 350.67px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 350.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c27 c2"></span></p><p class="c3"><span class="c27 c2">3 Method</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">1. Feature extraction from open data sources: Several </span><span class="c2">features, such as nightlight intensity or distance to schools, are extracted from the open data sources for each location that has been surveyed. All of the features are used as covariates in a regularized linear model (Ridge Regression) to make predictions. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">2. Geo-referenced survey data: </span><span class="c2">In order t</span><span class="c2">o take into account the spatial relationships in the survey data (a food-insecure household is more likely to be close to other food-insecure households rather than in the middle of a wealthy population)</span><span class="c2">,</span><span class="c2">&nbsp;we separately </span><span class="c2">run </span><span class="c2">a k-Nearest Neighbor algorithm on the GPS coordinates. With some datasets, this interpolation technique actually leads to better results than </span><span class="c2">the </span><span class="c2">regression that is trained on the extracted features</span><span class="c2">&nbsp;</span><span class="c2">(when other data sources are excluded)</span><span class="c2">. </span><span class="c2">Our results showed that</span><span class="c2">&nbsp;more sophisticated interpolation techniques such as Kriging </span><span class="c2">do not always</span><span class="c5 c2">&nbsp;lead to better results. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">To receive the most detailed results, th</span><span class="c5 c2">e predictions from both models (the Ridge regression and the k-NN) are averaged in the final model.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Prediction models will be strongly biased if they do not take into account population density.</span><span class="c2">We </span><span class="c2">therefore </span><span class="c2">only score </span><span class="c2">inhabited</span><span class="c2">&nbsp;areas where the filtering is done </span><span class="c2">using </span><span class="c2">Worl</span><span class="c2">d</span><span class="c2">Pop&rsquo;s population densities rasters [4]. </span><span class="c2">Having tested different providers, w</span><span class="c2">e found WorldPop to be the most accurate source </span><span class="c2">for detecting</span><span class="c5 c2">&nbsp;settlements. &nbsp; </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">To test the validity of our models, w</span><span class="c2">e evaluate them on unseen data and report the coefficient of determination (R2). The cross-validated R2 scores are averaged from 20 different </span><span class="c2">five</span><span class="c2">-fold cross-validation loops to get </span><span class="c2">more reliable</span><span class="c2">&nbsp;results. In each fold, we also run nested cross-validation to tune the regularization (alpha) parameter of the </span><span class="c2">R</span><span class="c2">idge regression and the number of neighbo</span><span class="c2">u</span><span class="c5 c2">rs (k) in the k-NN regression.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">The prototype is built in python and open-source libraries. The code is publicly available on </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://github.com/WFP-VAM/HRM&amp;sa=D&amp;ust=1534154057935000">GitHub</a></span><span class="c2">&nbsp;</span><span class="c5 c2">and more complex algorithms can easily be integrated. </span></p><p class="c3"><span class="c5 c2">&nbsp;</span></p><p class="c3"><span class="c5 c2">&nbsp;</span></p><h1 class="c37" id="h.wuf7cbh0yoe9"><span class="c2">4</span><span class="c27 c2">&nbsp;Sources of Information </span></h1><h2 class="c11" id="h.1g0vftrc668b"><span class="c36 c2">4.1 Ground Truth</span></h2><p class="c3"><span class="c2">To test our approach, we have used household</span><span class="c2">-level</span><span class="c2">&nbsp;survey data from three different </span><span class="c2">types of data-sets</span><span class="c2">&nbsp;which each collect data on different indicators</span><span class="c2">. We tested our models for both general poverty and food security indicators, as available in the data:</span></p><ul class="c42 lst-kix_xvwfwnxcnqtq-0 start"><li class="c3 c29"><span class="c2">World Bank </span><span class="c2">(WB) Living Standards Measurement Study (</span><span class="c2">LSMS</span><span class="c2">)</span><span class="c5 c2">: Total and food expenditures.</span></li><li class="c3 c29"><span class="c2">WFP Surveys: </span><span class="c2">Food Consumption Score (</span><span class="c2">FCS</span><span class="c2">)</span><span class="c2">&nbsp;and </span><span class="c2">t</span><span class="c2">otal </span><span class="c2">e</span><span class="c2">xpenditures (lighter module</span><span class="c2">&nbsp;than LSMS</span><span class="c5 c2">).</span></li></ul><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">In order to provide a reliable estimate of the indicator at the community level, data was aggregated to the lowest community level available, the cluster-level. </span><span class="c2">In WB and DHS data, </span><span class="c2">geolocation is available at the cluster level</span><span class="c2">, but</span><span class="c2">&nbsp;set off </span><span class="c2">(up to 10km) for data privacy reasons. In </span><span class="c2">the </span><span class="c2">WFP data, we have access to household</span><span class="c2">-</span><span class="c2">level GPS coordinates. </span><span class="c2">The interviews for most of the recent surveys have been conducted using tablets</span><span class="c2">&nbsp;that automatically report the coordinates of each interview. These locations are aggregated at the cluster (enumeration area) level. When the cluster variable is not available, a clustering</span><span class="c2">&nbsp;on geo-location</span><span class="c2">&nbsp;is performed. Clusters with less than </span><span class="c2">three</span><span class="c2">&nbsp;households are discarded</span><span class="c2">&nbsp;to reduce the influence of possible outliers</span><span class="c2">. </span><span class="c2">The number of households per cluster varies from </span><span class="c2">one</span><span class="c2">&nbsp;assessment to another but is in most cases between 10 and 30</span><span class="c2">.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Indicators are aggregated on cluster-level as mean (FCS, wealth index) or median (expenditures), depending on the existence of outliers. Total and food expenditures are continuous variables, with food expenditures being one component of total expenditures. Comparison between WFP and WB expenditures indicators is difficult due to a shorter survey module in WFP surveys. The wealth index is a composite measure of a household&rsquo;s living standard. It is constructed using Principal Component Analysis (PCA) of information about asset ownership. The Food Consumption Score (FCS) is one of WFPs corporate proxy indicator for household food security. It measures dietary diversity and meals</span><span class="c2">&nbsp;frequency</span><span class="c2">.</span></p><h2 class="c11" id="h.qctiwc8ileu"><span class="c2">4</span><span class="c2">.2 </span><span class="c2">Features extraction from open sources</span></h2><h3 class="c10" id="h.jk1dfrdnz1t"><span class="c31 c2">4.2.1 Transfer Learning</span></h3><p class="c3"><span class="c2">One of the data source</span><span class="c2">s</span><span class="c2">&nbsp;used in the pipeline are RGB images for the AOI from Sentinel 2 and Google Maps</span><span class="c2">. The processing here is inspired by the transfer learning approach developed by Neal et al. [2]</span><span class="c2">:</span><span class="c2">&nbsp;convolutional neural networks (CNNs) are trained to predict nightlight intensities (as a proxy of poverty) from satellite imagery and later used to extract</span><span class="c2">&nbsp;features</span><span class="c5 c2">&nbsp;to be used as covariates in the same pipeline mentioned above. In our pipeline we train two models respectively: One for Google images and one for Sentinel images.</span></p><h4 class="c41" id="h.ra31jgogyz1j"><span class="c2">4</span><span class="c40 c2">.2.1.1 Training the CNN on nightlights data</span></h4><p class="c3"><span class="c2">The nightlights data is obtained from the same source as described </span><span class="c2">below in the Section</span><span class="c2">&nbsp;4.5 </span><span class="c2">and then </span><span class="c2">binned</span><span class="c2">&nbsp;into three categories according to their luminosity: low, medium and high. These values become the </span><span class="c2 c15">labels </span><span class="c2">for training our models. The nightlights data from the following countries has been used so far to train the model: Senegal, Nigeria, Uganda and Malawi. The nightlights are masked with ESA&rsquo;s land use </span><span class="c2">product </span><span class="c2">to take luminosity only from populated areas in order to reduce </span><span class="c2">class imbalance</span><span class="c5 c2">. Google and Sentinel images are then downloaded.</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 356.00px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 356.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34"><span class="c15">Fig X Nightlights binned as low (reg), medium (yellow) and high (green) in northern Nigeria.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">The images and the nightlights </span><span class="c2">classes </span><span class="c2">are fed to convolutional neural networks. Compared to the transfer learning approaches mentioned above, our method uses a much smaller model architecture for two reasons: firstly, because we used fewer images and secondly, because we wanted a light model that would be able to score images fast. The final accuracy on validation sets ranges from 60% to 70%. The code for this component including the network&rsquo;s architecture can be found </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://github.com/WFP-VAM/HRM_NN_Training&amp;sa=D&amp;ust=1534154057941000">here</a></span><span class="c5 c2">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 230.85px; height: 210.50px;"><img alt="" src="images/image8.png" style="width: 230.85px; height: 210.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h4 class="c41" id="h.2wn1j0yr1ngm"><span class="c2">4</span><span class="c2 c40">.2.1.2 Features Extraction</span></h4><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 543.00px; height: 428.00px;"><img alt="" src="images/image1.png" style="width: 543.00px; height: 428.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2">The trained models are then used to extract </span><span class="c2">features </span><span class="c2">from the satellite images relevant to the cluster tha</span><span class="c2">t</span><span class="c2">&nbsp;needs to be/has been surveyed. In this configuration we remove the last layer of the network that is used to predict the </span><span class="c2">three</span><span class="c2">&nbsp;classes and average the outputs resulting in 256 features per image per satellite. D</span><span class="c2">imensionality reduction (PCA) is applied to these features to obtain ten features per image per satellite.</span><span class="c2">&nbsp;The class that handles this work in the code is the </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://github.com/WFP-VAM/HRM/blob/master/Src/nn_extractor.py&amp;sa=D&amp;ust=1534154057942000">nn_extractor</a></span><span class="c5 c2">.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><h2 class="c11" id="h.jcshz3fttdxm"><span class="c2">4</span><span class="c36 c2">.3 OpenStreetMap features</span></h2><p class="c3 c4"><span class="c5 c21"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 410.67px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 410.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34"><span class="c15">Fig X Schools and hospitals tagged in Open Street Map in parts of Kinshasa.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c5 c2">Open Street Map (OSM) is often the most accurate source of data on infrastructure in developing countries. All the data can be easily accessed and downloaded through the OverPass API. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">We currently extract the location of schools and hospitals in the AOI from Open Street Map and compute the distance of the AOI to the closest school and hospital. Other infrastructure locations could be extracted such as parks, roads, and trees. However, the completeness of these layers varies a lot from country to country and </span><span class="c2">even </span><span class="c2">within the same country. We found that schools and hospitals have been the features most thoroughly mapped by OSM volunteers across </span><span class="c2">our </span><span class="c5 c2">countries of interest. &nbsp;</span></p><p class="c3 c4"><span class="c5 c2"></span></p><h2 class="c11" id="h.ln1b8ngf4cm"><span class="c2">4</span><span class="c2">.4 Remote Sensing </span><span class="c2">Ind</span><span class="c2">ices</span></h2><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 199.52px; height: 176.50px;"><img alt="" src="images/image4.png" style="width: 199.52px; height: 176.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 200.43px; height: 175.50px;"><img alt="" src="images/image7.png" style="width: 200.43px; height: 175.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 199.50px; height: 176.15px;"><img alt="" src="images/image10.png" style="width: 199.50px; height: 176.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34"><span class="c15">Fig X max NDVI, NDWI and NDBI extarcted from Sentinel 2 in Kinshasa .</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c5 c2">Sentinel-2 is an Earth observation mission of the European Space Agency (ESA) launched in 2015. Multi-spectral images of the earth are available every five days at a resolution of ten metres. We use the Google Earth Engine platform to access and compute indices on these imageries. Currently, we extract three indices from four spectral bands (green, red, near infrared and shortwave infrared bands) at a given x, y location:</span></p><p class="c3 c4"><span class="c5 c2"></span></p><ul class="c42 lst-kix_h256d4t1o6ma-0 start"><li class="c3 c29"><span class="c21">NDVI: </span><span class="c2">The</span><span class="c21">&nbsp;</span><span class="c5 c2">Normalized Difference Vegetation Index is computed from the Red and Near-Infrared bands using the following formula : NDVI = (NIR - R) / (NIR + R). NDVI is used globally by the scientific community to monitor the health of vegetation </span></li><li class="c3 c29"><span class="c21">NDWI: </span><span class="c5 c2">The Normalized Difference Water Index is computed from the Green and Infrared bands using the following formula: NDWI = (G - NIR) / (G + NIR)</span></li><li class="c3 c29"><span class="c21">NDBI: </span><span class="c2">T</span><span class="c2">he Normalized Difference Built-up Index is computed from the Panchromatic and Shortwave Infrared (SWIR) and Near-Infrared bands using the following formula: NDBI = (NIR - SWIR) / (NIR + SWIR)</span></li></ul><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">We compute the maximum values for NDVI, NDWI and NDBI</span><span class="c2">&nbsp;</span><span class="c2">in a twelve-month period corresponding to the year of data collection. We then average the indexes spatially in a 1 kilometre x 1 kilometre buffer around the point of interest. A</span><span class="c2">s the Sentinel-2 data only started to be available in late 2015, if the survey data was collected before this date, we take the 2016 indices instead.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Taking the maximum value over a year gives us a greater chance of extracting these indices in images without clouds. For the </span><span class="c2">vegetation index</span><span class="c5 c2">, this technique also allows us to have a picture of the state of the crops at the peak of the growing season. More research needs to be done on extracting seasonality NDVI indices that reflect the difference between the growing seasons and the rest of the agricultural calendar or compare the survey year with the previous years to detect a bad or good growing season that could partially explain the food security status. &nbsp;</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><h2 class="c11" id="h.nfbkw7inunoa"><span class="c2">4</span><span class="c2">.5 Nightlights</span></h2><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 430.67px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 430.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34"><span class="c15">Fig X Luminosity over Kinshasa in 2017.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Nightlights are highly correlated with poverty (Neal, J.) and from our experience, to some extent with food security. For each AOI and relevant period, we derive the luminosity from the </span><span class="c2"><a class="c9" href="https://www.google.com/url?q=http://www.noaa.gov/&amp;sa=D&amp;ust=1534154057947000">National Oceanic and Atmospheric Administration </a></span><span class="c2"><a class="c9" href="https://www.google.com/url?q=http://www.noaa.gov/&amp;sa=D&amp;ust=1534154057948000">(NOAA) </a></span><span class="c2"><a class="c9" href="https://www.google.com/url?q=http://www.noaa.gov/&amp;sa=D&amp;ust=1534154057948000">of the US Department of Commer</a></span><span class="c2"><a class="c9" href="https://www.google.com/url?q=http://www.noaa.gov/&amp;sa=D&amp;ust=1534154057948000">ce</a></span><span class="c2 c49"><a class="c9" href="https://www.google.com/url?q=http://www.noaa.gov/&amp;sa=D&amp;ust=1534154057948000">.</a></span><span class="c2">&nbsp;In particular we use the N</span><span class="c2">OAA Visible Infrared Imaging Radiometer Suite (VIIRS) monthly product, normalized by the DMSP-OLS Nighttime Lights Time Series, also from NOAA.</span><span class="c2">&nbsp;The result is a georeferenced dataset with the luminosity shown on a 100 metre </span><span class="c2">x</span><span class="c2">&nbsp;100 metre grid. Each surveyed cluster is then assigned an average luminosity from the area around its location. The core of the code that handles this data source is the </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://github.com/WFP-VAM/HRM/blob/master/Src/nightlights.py&amp;sa=D&amp;ust=1534154057948000">nightlights class</a></span><span class="c5 c2">.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3 c4"><span class="c5 c2"></span></p><h1 class="c37" id="h.33mrj8ohbtxh"><span class="c2">5</span><span class="c2">&nbsp;Results</span></h1><p class="c3"><span class="c2">Our r</span><span class="c2">esults on </span><span class="c2">the chosen poverty and food security</span><span class="c2">&nbsp;indicators are promising</span><span class="c2">. </span><span class="c2">We are using </span><span class="c2">the</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html&amp;sa=D&amp;ust=1534154057950000">&nbsp;</a></span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html&amp;sa=D&amp;ust=1534154057950000">R2</a></span><span class="c2">&nbsp;as the</span><span class="c2">&nbsp;evaluation metric because it gives us</span><span class="c2">&nbsp;a sense of the performance of the model against the </span><span class="c2">mean</span><span class="c2">. The datasets we have used to test the method are from a variety of countries in Africa and </span><span class="c2">were </span><span class="c2">collected </span><span class="c2">by </span><span class="c2">a number of </span><span class="c2">organisations</span><span class="c2">. The two main sources are described in Section 4.1: LSMS surveys from the World Bank </span><span class="c2">and the </span><span class="c2">World Food Programme&rsquo;s </span><span class="c2">main survey</span><span class="c2">,</span><span class="c2">&nbsp;the</span><span class="c2">&nbsp;</span><span class="c2">Enqu&ecirc;te Nationale de la S&eacute;curit&eacute; Alimentaire (ENSA)</span><span class="c2">&nbsp;as well as </span><span class="c2">the Comprehensive Food Security and Vulnerability Analysis (CFSVA). </span><span class="c2">The model was also tested on data from one WFP urban food security assessment. </span><span class="c2">From</span><span class="c2">&nbsp;all surveys we </span><span class="c2">used expenditures as the proxy</span><span class="c2">&nbsp;</span><span class="c2">indicator for </span><span class="c2">&nbsp;poverty </span><span class="c2">as well as</span><span class="c2">&nbsp;one </span><span class="c2">proxy indicator</span><span class="c2">&nbsp;</span><span class="c2">for</span><span class="c2">&nbsp;food security (</span><span class="c2">food expenditures or FCS, depending on availability.</span><span class="c2">). </span><span class="c2">The table b</span><span class="c2">elow </span><span class="c2">presents </span><span class="c2">&nbsp;</span><span class="c2">the </span><span class="c5 c2">results for the principal datasets that we have been evaluating the model on:</span></p><p class="c3 c4"><span class="c5 c2"></span></p><a id="t.d441a3f4d92cd3ee4ef63ebb0ec4516d85920115"></a><a id="t.0"></a><table class="c33"><tbody><tr class="c19"><td class="c14" colspan="1" rowspan="1"><p class="c8"><span class="c1">Survey Type</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c1">AOI</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c1">Clusters</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c8"><span class="c1">Year</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Indicator</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c8"><span class="c1">R2</span></p></td></tr><tr class="c19"><td class="c14" colspan="1" rowspan="6"><p class="c8"><span class="c1">LSMS</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c1">Nigeria</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c1">408</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c23"><span class="c1">2012</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.42</span></p></td></tr><tr class="c19"><td class="c26" colspan="1" rowspan="2"><p class="c8"><span class="c1">Malawi</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c8"><span class="c1">205</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c23"><span class="c1">2013</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.25</span></p></td></tr><tr class="c19"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.18</span></p></td></tr><tr class="c19"><td class="c26" colspan="1" rowspan="2"><p class="c8"><span class="c1">Tanzania</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c8"><span class="c1">409</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c23"><span class="c1">2012</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.38</span></p></td></tr><tr class="c19"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.34</span></p></td></tr><tr class="c19"><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c1">Uganda</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c1">641</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c23"><span class="c1">2013</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.46</span></p></td></tr><tr class="c19"><td class="c14" colspan="1" rowspan="2"><p class="c8"><span class="c1">CFSVA</span></p></td><td class="c26" colspan="1" rowspan="2"><p class="c8"><span class="c1">Sierra Leone</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c8"><span class="c1">1292</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c23"><span class="c1">2015</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.37</span></p></td></tr><tr class="c19"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Consumption Score</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.24</span></p></td></tr><tr class="c19"><td class="c14" colspan="1" rowspan="2"><p class="c8"><span class="c1">URBAN</span></p></td><td class="c26" colspan="1" rowspan="2"><p class="c8"><span class="c1">Kinshasa</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c8"><span class="c1">159</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c23"><span class="c1">2017</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.25</span></p></td></tr><tr class="c19"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Consumption Score</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.18</span></p></td></tr><tr class="c19"><td class="c14" colspan="1" rowspan="4"><p class="c6"><span class="c1">CFSVA</span></p></td><td class="c26" colspan="1" rowspan="2"><p class="c6"><span class="c1">Senegal</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c6"><span class="c1">1207</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c12"><span class="c1">2013</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.27</span></p></td></tr><tr class="c50"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Consumption Score</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.52</span></p></td></tr><tr class="c19"><td class="c26" colspan="1" rowspan="2"><p class="c6"><span class="c1">Burkina Faso</span></p></td><td class="c20" colspan="1" rowspan="2"><p class="c6"><span class="c1">567</span></p></td><td class="c17" colspan="1" rowspan="2"><p class="c12"><span class="c1">2018</span></p></td><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Expenditures (log)</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.12</span></p></td></tr><tr class="c19"><td class="c25" colspan="1" rowspan="1"><p class="c8"><span class="c1">Food Consumption Score</span></p></td><td class="c24" colspan="1" rowspan="1"><p class="c23"><span class="c1">0.34</span></p></td></tr></tbody></table><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 340.16px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 340.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c11" id="h.vbfldnp3qoxf"><span class="c2 c44">The table shows that for expen</span><span class="c5 c2">ditures (as a proxy for poverty) from LSMS data we can reach an R2 of 40-50%, depending on the country, with our method. For expenditures from WFP data, results are generally a little lower, which might be due to a regional concentration of vulnerable areas in these surveys, as well as due to a less detailed survey module.</span></h2><p class="c3"><span class="c5 c2">For food security indicators, namely food expenditures in World Bank and Food Consumption Score in WFP datasets, results are generally some percentage points lower than for expenditures in the respective country. An exception can be observed in Senegal and Burkina Faso (both WFP data), where the R2 for FCS is higher than the one for expenditures, reaching the best result of all predictions with 52% in Senegal. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">When interpreting these results, we need to take into account the different quality of indicators in LSMS and WFP datasets. While LSMS surveys have a strong focus on expenditures and use a very detailed module, WFP surveys are shorter and less detailed and precise in terms of expenditures. On the other hand, the indicator on food security in WFP surveys, FCS, is better suited to actually measuring food access through food diversity and quality than the LSMS indicator of food expenditures. With regards to this trade-off, the results from Senegal and Burkina Faso in particular show the potential that our method offers for predicting food security indicators at a high resolution. To verify the potential, we are currently testing the model on further data sets taking into account the different contexts of those countries. </span></p><h2 class="c11" id="h.qty3w9bhp6hu"><span class="c2">5</span><span class="c36 c2">.1 Problems and Limitations</span></h2><h3 class="c10" id="h.1s4nh232cr1y"><span class="c2">5</span><span class="c31 c2">.1.1 Ground truth data</span></h3><p class="c3"><span class="c2">We have found survey data to be a challenging aspect of this project. Processing and preparing survey data is very time</span><span class="c2">-</span><span class="c2">consuming as datasets differ in nomenclature and methods. Furthermore, to this date, geographic locations of the surveys have hardly been recorded accurately, if at all, and sometimes are not made public due to the sensitivity of the data.</span><span class="c2">&nbsp;The study is therefore limited to only a few available recent datasets. </span></p><p class="c3 c4"><span class="c5 c2"></span></p><h3 class="c10" id="h.wbrh31ps82uo"><span class="c2">5.</span><span class="c2 c31">1.2 Data Homogeneity</span></h3><p class="c3"><span class="c2">The data sources used in this application are chosen for their accessibility independently of the dataset. </span><span class="c2">This makes the method scalable as all sources can be accessed programmatically,</span><span class="c2">&nbsp;just based on geo locations. However, depending on the dataset we are processing, different sources may be more or less useful to the model: some cities and regions in OpenStreetMap for example are very well tagged (for example Kinshasa), however entire other countries may be lacking satisfactory tagging (for example Burkina Faso). Similarly, the nightlight intensities, as previously shown by </span><span class="c2">[2]</span><span class="c2">, </span><span class="c2">correlate well</span><span class="c2">&nbsp;with poverty in better-off areas but are almost non-existent in many areas that are the focus of WFP&rsquo;s work (</span><span class="c2">such as </span><span class="c5 c2">central Africa).</span></p><p class="c3"><span class="c2">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 485.12px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 485.12px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c11" id="h.pwg080ld9wpj"><span class="c2">5</span><span class="c36 c2">.2 Future Work</span></h2><h3 class="c10" id="h.hx8re82vlwky"><span class="c2">5</span><span class="c31 c2">.2.1 Open Source and Productionisation</span></h3><p class="c3"><span class="c2">We want this solution or parts of it (for example modules dedicated to extracting OSM features) to be easily </span><span class="c2">usable</span><span class="c2">, by us and the wider community. The codebase is already public, however, we will invest time </span><span class="c2">in</span><span class="c2">to refactor</span><span class="c2">ing</span><span class="c2">&nbsp;the code, improv</span><span class="c2">ing</span><span class="c2">&nbsp;and document</span><span class="c2">ing</span><span class="c2">&nbsp;the method</span><span class="c2">&nbsp;</span><span class="c2">and mak</span><span class="c2">ing</span><span class="c5 c2">&nbsp;deployment easier.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><p class="c3"><span class="c2">At the moment, there are a few bottlenecks that slow down the application. The process that takes longest is the downloading and processing of Sentinel images because of their size (much larger than Google Maps). We have already moved most of the code that concerns Sentinel data to the Google Earth Engine API because we find this access point more convenient than the </span><span class="c2">one provided by </span><span class="c2">SentinelHUB</span><span class="c2">.</span><span class="c2">&nbsp;</span><span class="c2">H</span><span class="c2">owever, with ~1 second per image</span><span class="c2">,</span><span class="c2">&nbsp;the system is still too slow </span><span class="c2">when processing</span><span class="c5 c2">&nbsp;large datasets.</span></p><h3 class="c10" id="h.unpu9vo28qr5"><span class="c2">5</span><span class="c31 c2">.2.2 Better Proxy for Poverty</span></h3><p class="c3"><span class="c2">Replac</span><span class="c2">ing</span><span class="c2">&nbsp;nightlights with another proxy (maybe Call Detail Records?) to train the nets</span><span class="c2">&nbsp;might improve our predictions</span><span class="c5 c2">.</span></p><p class="c3 c4"><span class="c5 c2"></span></p><h1 class="c37" id="h.za20ipz7hd1h"><span class="c27 c2">References</span></h1><p class="c6 c18"><span class="c2">Alegana, V. A., Atkinson, P. M., Pezzulo, C., Sorichetta, A., Weiss, D., Bird, T., &hellip; Tatem, A. J. (2015). Fine resolution mapping of population age-structures for health and development applications. </span><span class="c2 c15">Journal of The Royal Society Interface</span><span class="c2">, </span><span class="c2 c15">12</span><span class="c2">(105).</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1098/rsif.2015.0073&amp;sa=D&amp;ust=1534154057978000">&nbsp;</a></span><span class="c0"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1098/rsif.2015.0073&amp;sa=D&amp;ust=1534154057978000">https://doi.org/10.1098/rsif.2015.0073</a></span></p><p class="c6 c18"><span class="c2">Elbers, C., Lanjouw, J. O., &amp; Lanjouw, P. (2003). Micro-Level Estimation of Poverty and Inequality. </span><span class="c2 c15">Econometrica</span><span class="c2">, </span><span class="c2 c15">71</span><span class="c2">(1), 355&ndash;364.</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1111/1468-0262.00399&amp;sa=D&amp;ust=1534154057978000">&nbsp;</a></span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1111/1468-0262.00399&amp;sa=D&amp;ust=1534154057979000">https://doi.org/10.1111/1468-0262.00399</a></span></p><p class="c6 c18"><span class="c2">Giorgi, E. &amp; Diggle, P (2016). Model-Based Geostatistics for Prevalence Mapping in Low-Resource Settings. </span><span class="c2 c15">Journal of the American Statistical Association</span><span class="c2">, 111:515, 1096-1120, </span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1080/01621459.2015.1123158&amp;sa=D&amp;ust=1534154057979000">https://doi.org/10.1080/01621459.2015.1123158</a></span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1080/01621459.2015.1123158&amp;sa=D&amp;ust=1534154057979000">&nbsp;</a></span></p><p class="c6 c18"><span class="c2">Head, A., Manguin, M., Tran, N., &amp; Blumenstock, J. E. (2017). Can Human Development be Measured with Satellite Imagery? (pp. 1&ndash;11). ACM Press.</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1145/3136560.3136576&amp;sa=D&amp;ust=1534154057980000">&nbsp;</a></span><span class="c0"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1145/3136560.3136576&amp;sa=D&amp;ust=1534154057980000">https://doi.org/10.1145/3136560.3136576</a></span></p><p class="c6 c18"><span class="c2">Jean, N., Burke, M., Xie, M., Davis, W. M., Lobell, D. B., &amp; Ermon, S. (2016). Combining satellite imagery and machine learning to predict poverty. </span><span class="c2 c15">Science</span><span class="c2">, </span><span class="c2 c15">353</span><span class="c2">(6301), 790.</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1126/science.aaf7894&amp;sa=D&amp;ust=1534154057980000">&nbsp;</a></span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1126/science.aaf7894&amp;sa=D&amp;ust=1534154057980000">https://doi.org/10.1126/science.aaf7894</a></span></p><p class="c6 c18"><span class="c2">Steele, J. E., Sunds&oslash;y, P. R., Pezzulo, C., Alegana, V. A., Bird, T. J., Blumenstock, J., &hellip; Bengtsson, L. (2017). Mapping poverty using mobile phone and satellite data. </span><span class="c2 c15">Journal of the Royal Society, Interface</span><span class="c2">, </span><span class="c2 c15">14</span><span class="c2">(127).</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1098/rsif.2016.0690&amp;sa=D&amp;ust=1534154057981000">&nbsp;</a></span><span class="c0"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1098/rsif.2016.0690&amp;sa=D&amp;ust=1534154057981000">https://doi.org/10.1098/rsif.2016.0690</a></span></p><p class="c6 c18"><span class="c2">Tatem, A. J. (2014). Mapping the denominator: spatial demography in the measurement of progress. </span><span class="c2 c15">International Health</span><span class="c2">, </span><span class="c2 c15">6</span><span class="c2">(3), 153&ndash;155.</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1093/inthealth/ihu057&amp;sa=D&amp;ust=1534154057981000">&nbsp;</a></span><span class="c0"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1093/inthealth/ihu057&amp;sa=D&amp;ust=1534154057982000">https://doi.org/10.1093/inthealth/ihu057</a></span></p><p class="c6 c18"><span class="c2">Wardrop, N. A., Jochem, W. C., Bird, T. J., Chamberlain, H. R., Clarke, D., Kerr, D., &hellip; Tatem, A. J. (2018). Spatially disaggregated population estimates in the absence of national population and housing census data. </span><span class="c2 c15">Proceedings of the National Academy of Sciences</span><span class="c2">, </span><span class="c2 c15">115</span><span class="c2">(14), 3529.</span><span class="c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1073/pnas.1715305115&amp;sa=D&amp;ust=1534154057982000">&nbsp;</a></span><span class="c16 c2"><a class="c9" href="https://www.google.com/url?q=https://doi.org/10.1073/pnas.1715305115&amp;sa=D&amp;ust=1534154057983000">https://doi.org/10.1073/pnas.1715305115</a></span></p><p class="c6 c18 c4"><span class="c2 c15 c35"></span></p><div><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 133.50px; height: 36.41px;"><img alt="" src="images/image11.jpg" style="width: 133.50px; height: 36.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></div></body></html>